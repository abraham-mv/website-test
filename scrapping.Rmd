---
title: "The Origins of Formula 1: (Part 1) Webscrapping"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Formula 1 data
In the modern era, the sport generates a lot of data, each car has between 150 and 300 sensor, generating an approximate of 300gb of data per car per grand prix weekend. This is of great use for the engineers who can take data-driven decisions in order to extract the maximum performance out of the car on each event; however, this wasnâ€™t always the case. In the early days of the sport it was technologically impossible to generate or store the amounts of data we have today. \
This data is publicly available in a tidy format in some websites, APIs and python packages, such as, the `fastf1` python library. Early formula 1 data is available on the FIA's website, mostly in the form of html tables. If we want to analyze this data we will have to scrape it. 

## Webscrapping code

We will import the [`tidyverse`](https://www.tidyverse.org/) library, which is a collection of packages very useful for data wrangling. The main package we'll use for webscrapping is the `rvest` package, we will also need the `stringr` package, which allows us to manipulate more easily string-type variables.

```{r include=FALSE}
library(tidyverse)
library(rvest)
```

We will store our main site links from where the scrapping will start.
```{r}
link_fia <- "https://fiaresultsandstatistics.motorsportstats.com"
link_seasons <- "https://fiaresultsandstatistics.motorsportstats.com/series/formula-one/season/"
```

If we add a specif year to the `link_seasons` string, it will take us to the main page of that F1 season. The `read_html`, extracts all html code from the site, from there we can retrieve specific nodes. Let's go to the link of the first Formula 1 grand prix ever.

```{r}
link_british_1950 <- "https://fiaresultsandstatistics.motorsportstats.com/results/1950-british-grand-prix/classification"
british_1950_page <- read_html(link_british_1950)
british_1950_race <- british_1950_page |> 
  html_table()

british_1950_race
```
Here the `html_table` function retrieves a list of tibbles of all html tables on the page, fortunately, in this case there was only table. The tibble consists of the position a certain driver finished, the laps ran, the total race time, average speed, and their fastest lap of the race and in which lap was completed, as well as, the drivers' team and nationality.  If we navigate to the site, we can see that there's a menu right above the race positions table, which has the buttons: "Qualifying", "Grid" and "Race", these buttons have hyperlinks which will take you to another page. We already have the race information, but we are also interested in the qualifying data, we know that in html the hyperlinks are stored in "a" nodes, so we can use the following code to retrieve the link to the qualifying page.
```{r}
href_qual <- british_1950_page |> 
  html_node(xpath = "//a[text() = 'Qualifying']") |> 
  html_attr("href")

british_1950_qual_link <- paste0(link_fia, href_qual)

british_1950_qual_link
```
Here we are asking `R` to retrieve the "a" html node that contains the text "Qualifying", and get the hyperlink of that node, then we paste that to the url for our main site. We can use the same function as with race data, to get a tibble of qualifying positions.
```{r}
british_1950_qual <- read_html(british_1950_qual_link) |> 
  html_table()

british_1950_qual
```
Above the Qualifying, Grid and Race menu there's another menu with buttons: "Event Info", "Classification", "Session Facts" and "Standings". The race and qualifying data are under the classification page; however, under event info. there's information about the engine and car each driver was running, which might be important in other analysis. We can use the same trick to get the link to this page, and then retrieve the table. 
```{r}
href_event <- british_1950_page |> 
  html_nodes(xpath = "//a[text() = 'Event Info']") |> 
  html_attr("href")

british_1950_event <- read_html(paste0(link_fia, href_event)) |> 
  html_table()

british_1950_event
```
Under the "Session Facts" section there's a lap by lap positions chart, which might be useful to retrieve. The horizontal axis of this chart are the race positions, while the vertical axis has the lap numbers, while the cell value is the driver's number.
```{r}
href <- british_1950_page |> 
  html_nodes(xpath = "//a[text() = 'Session Facts']") |> 
  html_attr("href")

href2 <- read_html(paste0(link_fia, href)) |> 
  html_nodes(xpath = "//a[text() = 'Lap Chart']") |> 
  html_attr("href")

lap_chart_link <- paste0(link_fia, href2)
lap_chart_link

read_html(lap_chart_link) |> 
  html_table()
```
If we go to the site we can see that there's a lap by lap positions table, but this can't be retrieved using the `html_table` function, so we will have to go a little deeper into the html code. Using inspect mode in the web browser we see that the elements of the chart are under div nodes with a specif class:
```{r}
lap_chart_page <- read_html(lap_chart_link) 

div_nodes <- lap_chart_page |> 
  html_nodes(xpath = "//div[@class='_1BvfV']")

div_nodes  
```
If this was a regular square chart, we could just use the `html_text` function to retrieve a character array of the text inside the div nodes and then convert it to a matrix; however, if we take a look at the chart on the site we can see that it has an irregular form since some drivers retire from the race, so the number of columns (positions in the race) is constantly shrinking. This leaves a lot empty cells in the table, i.e div nodes with no text attribute, therefore they can't be retrieve with the `html_text` function. \
To work around this issue we will have to consider each div node as a character variable, from the above output we can see that all nodes follow the same pattern. We will use the `regexpr` function to extract the values between `<div>` and `<\div>`, if there's nothing between them, we can just store an empty string.
```{r}
div_char <- as.character(div_nodes)
my_numbers <- character(length(div_char))

for (k in seq_along(div_char)){
  match <- regexpr("(?<=<div class=\"_1BvfV\">)\\d*(?=</div>\\n)", div_char[k], perl = TRUE)
    if (match > 0) { # If there's something there store it in the vector
      my_numbers[k] <- substr(div_char[k], match, match + attr(match, "match.length") - 1)
    } else { # If there isn't anything there store an empty value
      my_numbers[k] <- " "
    }
}
head(my_numbers, 20)
```
The next step is to build a tibble from this array, for that we need to retrieve the number of drivers who started the race, which is the same as the number of columns of our chart. There we use the inspect mode again to retrieve the div class in which the column names are stored. We extract the text under these nodes, and pull the last value.
```{r warning=FALSE}
total_drivers <- read_html(lap_chart_link) |> 
      html_nodes(xpath = "//div[@class='_3DVzL']") |> 
      html_text() |> tail(1) |> as.numeric()
# Make the character vector a matrix of total_drivers columns, just realize that not all charts 
lap_chart <- as_tibble(matrix(my_numbers, ncol = total_drivers) ) 
lap_chart
```
Thank you for reading this webscrapping exercise, hope you find it useful. If you want to see the full code and subsequent analysis check out my github repo https://github.com/abraham-mv/formula-1.
