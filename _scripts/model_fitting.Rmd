---
title: "The Origins of Formula 1: (Part 2) Bayesian Analysis"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data
In this post we will build and evaluate two bayesian models to predict the probability of a driver finishing in the top 5 of a given race. The data was scrapped from the FIA's website, check out my post LINK HERE, for more information on webscrapping. The raw data was later transformed using another script which you can find in my github repo LINK HERE. Specifically the following modifications were made: 
1. The Indy-500 race was dropped since most regular F1 drivers didn't participated in it.
2. Observations that had two drivers were separated into two rows, with all other variables equal. (If two drivers shared one car in a race and scored points, these were divided equally between them).
3. Make sure a driver only appear once in a grand prix, if he had two or more entries the one with the most amount of laps was kept.
3. Lap time, total race time, and best lap of the race were all character variables, which were transformed to total seconds.
4. Race position contained "DNF" (Did Not Finish) indicators, which were set to 99, and missing values too.
5. The engine variable was transformed to only display the manufacturer, leaving out model number and other additional information.
6. Teams and engines were transformed to factor variable, those that didn't appear more than 10 times in the whole dataset were set to "other". 
7. Teams that only had one distinct driver in the whole dataset were set to "solo".

For these transformed version of our dataset had 828 rows with 141 different drivers, 17 teams and 13 different engine manufacturers. \

First we need to load the necessary libraries
```{r}
library(tidyverse)
library(viridis)
library(rstan)
```


Before start fitting any models we need to load the data.
```{r include=T}

data_model <- read.csv("https://raw.githubusercontent.com/abraham-mv/formula-1/main/data/data_for_model.csv") |> select(-X)

```

In Formula 1, the drivers' ability isn't the most important factor on where will they finish a race, the machinery underneeth them is. That being said let's look at proportions of race positions by engine manufacturer.
```{r}
data_model |> 
mutate(engine_index = factor(engine_index),
         race_pos = ifelse(race_pos == 99, NA, race_pos)) |> 
  ggplot(aes(race_pos, fill = new_engine)) + 
   geom_bar(aes(y = ..prop..)) + 
  theme_bw(base_size = 12) + xlab("Race position") +
  ylab("Proportions") + ggtitle("Race positions by engine manufacturer") + 
  scale_fill_viridis(discrete = T, name=NULL) + 
  scale_x_continuous(breaks = seq(2,22,2)) + scale_y_continuous(breaks = seq(0,1.8,0.2)) 
```
We can see how dominant certain engine manufactures were in the early days of the sport, as the proportions of finishing first and second in the race were completely occupied by just four manufactures: Alfa Romeo, Ferrari, Maserati and Mercedes. Below we can see the distribution of 
```{r}
data_model |> 
  group_by(Driver) |> 
  mutate(appear = n()) |> 
  filter(appear > 10) |> 
  ggplot(aes(Kph, Driver, fill = Driver)) + geom_boxplot() + theme_bw() + 
  theme(legend.position = "none") + scale_fill_viridis(discrete = T)
```

## Candidate models
Given that the outcome variable is binary, we will fit a Bayesian logistic regression model (fixed effects). Also, based on the EDA, it might be a good idea to model some variables hierarchically (i.e mixed effects model).
The first Bayesian logistic regression is described by the following equation:
$$
\begin{aligned}
y_i | \theta_i &\sim \text{Bernoulli}(\theta_i) \\
\text{logit}(\theta_i) &= \beta_0 + \beta_1x_{\text{quali-pos}} + \beta_2x_{\text{avg-speed}} +  x^\top_{\text{engine}}\boldsymbol{\beta_3} + \beta_4x_{\text{champ-points}} \\
\beta_0,\dots,\beta_4&\sim N(0,1)
\end{aligned}
$$
For the mixed effects model, we will model the drivers, teams and engine manufacturers hierarchically. 
$$
\begin{aligned}
y_i | \theta_i &\sim \text{Bernoulli}(\theta_i) \\
\text{logit}(\theta_i) &= \alpha_0 + \alpha^{\text{driver}}_{d[i]}x_{\text{quali-pos}} + 
                          \alpha^{\text{team}}_{t[i]} + \alpha_{e[i]}^{\text{engine}}  \\
\alpha^{\text{driver}}_d &\sim N(\mu_d^{\text{driver}}, \sigma_d^{\text{driver}}) \\
\alpha^{\text{team}}_t &\sim N(\mu_t^{\text{team}}, \sigma_t^{\text{team}}) \\
\alpha^{\text{engine}}_e &\sim N(\mu_e^{\text{engine}}, \sigma_e^{\text{engine}}) \\
\mu_d^{\text{driver}}&\sim N(0,1)\\
\mu_d^{\text{team}}&\sim N(0,1)\\
\mu_d^{\text{engine}}&\sim N(0,1)\\
\sigma_d^{\text{driver}}&\sim N^+(0,1)\\
\sigma_d^{\text{team}}&\sim N^+(0,1)\\
\sigma_d^{\text{engine}}&\sim N^+(0,1)\\
\alpha_0 &\sim N(0,1) 
\end{aligned}
$$
- $y_i$: 1 if the driver at observation $i$ scored points, 0 otherwise.
- $x_{\text{quali-pos}}$: the position the driver qualified (started the race).
- $d[i]$: indicates who's the driver in row $i$ of the dataset; $d\in\{1,\dots,141\}$
- $t[i]$: indicates the team the driver in row $i$ is driving for; $t\in\{1,\dots,17\}$
- $e[i]$: indicates the engine the team in row $i$ is using; $e\in\{1,\dots,13\}$
Here we have two variable intercept corresponding the team and engine, and a coefficient for the qualifying position which varies with the driver.

## Running the models
To run the models we will use [`stan`](https://mc-stan.org/docs/stan-users-guide/index.html). Below is the code for the first one. Instead of specifying each feature individually, we can give stan a covariate matrix, and let it now that the beta coefficients are in vector format.
```{stan }
data {
  int<lower=0> N; // Number of observations
  int<lower=0> K; // Number of features
  int<lower=0, upper=1> y[N]; // Outcome variable
  matrix[N, K] X;
}
parameters {
  vector[K] beta; // Vector of coefficients
}
transformed parameters{
  vector[N] theta;
  theta = X * beta; // Specifying the theta parameter for the bernoulli dist.
}

model {
  beta ~ normal(0,1);
  y ~ bernoulli_logit(theta); 
}
generated quantities {
  // simulate data from the posterior
  vector[N] y_rep;
  // log-likelihood posterior
  vector[N] log_lik;
  for (i in 1:num_elements(y_rep)) {
    y_rep[i] = bernoulli_rng(inv_logit(theta[i]));
  }
  for (i in 1:num_elements(log_lik)) {
    log_lik[i] = bernoulli_logit_lpmf(y[i] | theta[i]);
  }
}
```

The second model is not a lot more complicated, we just need to feed stan with much more information, like number of drivers, teams and engines, and an id for all of those. It's important to make sure that all standard deviations are bigger than zero!
```{stan}
data {
  int<lower=0> N;  // number of observations
  int<lower=1> D;  // number of drivers
  int<lower=1> T;  // number of teams
  int<lower=1> E;  // number of engines
  int<lower=1> Y;  // number of years
  int driver[N];    // driver ID of each observation
  int team[N];      // team ID of each observation
  int engine[N];    // engine ID of each observation
  int grid[N];      // starting grid position of each observation
  int finished[N];  // binary outcome of each observation (0 = outside top 5, 1 = inside top 5)
  int num_races[N]; // number of races each driver participated in
}

parameters {
  real Intercept;
  vector[D] driver_effect;
  vector[T] team_effect;
  vector[E] engine_effect;
  real driver_mu;
  real<lower=0> driver_sd;
  real team_mu;
  real<lower=0> team_sd;
  real engine_mu;
  real<lower=0> engine_sd;
}

transformed parameters{
  vector[N] theta;
  for (n in 1:N){
    theta[n] = Intercept + driver_effect[driver[n]]  * grid[n] +
                       team_effect[team[n]] + engine_effect[engine[n]];
  }
}

model {
  // Priors
  Intercept ~ normal(0, 1);
  driver_effect ~ normal(driver_mu, driver_sd);
  team_effect ~ normal(team_mu, team_sd);
  engine_effect ~ normal(engine_mu, engine_sd);
  driver_mu ~ normal(0, 1);
  driver_sd ~ normal(0, 1);
  team_mu ~ normal(0, 1);
  team_sd ~ normal(0, 1);
  engine_mu ~ normal(0, 1);
  engine_sd ~ normal(0, 1);

  // Likelihood
  finished ~ bernoulli_logit(theta);
}

generated quantities {
  real y_rep[N];
  vector[N] log_lik;
  for (n in 1:N) {
    y_rep[n] = bernoulli_logit_rng(theta[n]);
    
    log_lik[n] = bernoulli_logit_lpmf(finished[n] | theta[n]);
  }
  
}
```
Now that we have the code for both our models we can run them from R.
```{r}
n_total <- length(data_model[,1])
quali_pos <- data_model$Pos
kph_cent <- data_model$Kph - mean(data_model$Kph)
engine <- data_model$new_engine
wdc_points <- data_model$wdc_points_shift
y <- data_model$points

covariates <- cbind(rep(1, n_total), quali_pos, kph_cent, model.matrix(~engine)[,-1], wdc_points)
stan_data <- list(
  N = n_total, K = dim(covariates)[2], 
  X = covariates,
  y = y
)
simple_mod <- stan(data = stan_data, file = here("model1.stan"),
                   chains = 4, iter = 1000, cores = 2)
```



